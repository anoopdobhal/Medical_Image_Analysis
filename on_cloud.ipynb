{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce18bf77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cloudml-hypertune in /opt/conda/lib/python3.7/site-packages (0.1.0.dev6)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install cloudml-hypertune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c135059",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3a91faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your current GCP Project Name is: qwiklabs-gcp-04-014eea1283f0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export PROJECT=$(gcloud config list project --format \"value(core.project)\")\n",
    "echo \"Your current GCP Project Name is: \"${PROJECT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0db7d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Change these to try this notebook out\n",
    "PROJECT = \"qwiklabs-gcp-04-014eea1283f0\"  # Replace with your PROJECT\n",
    "BUCKET = \"chest-xray-us-central\"   # defaults to PROJECT\n",
    "REGION = \"us-central1\"  # Replace with your REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d744b420",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"BUCKET\"] = BUCKET\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"TFVERSION\"] = \"2.1\"\n",
    "os.environ[\"PYTHONVERSION\"] = \"3.7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c723755",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [compute/region].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud config set project ${PROJECT}\n",
    "gcloud config set compute/region ${REGION}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a125d780",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if ! gsutil ls | grep -q gs://${BUCKET}; then\n",
    "    gsutil mb -l ${REGION} gs://${BUCKET}\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a9e54096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://chest-xray-us-central/chest_xray/chest_xray_labels.csv\n",
      "gs://chest-xray-us-central/chest_xray/test/\n",
      "gs://chest-xray-us-central/chest_xray/train/\n",
      "gs://chest-xray-us-central/chest_xray/val/\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gsutil ls gs://${BUCKET}/chest_xray/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21456727",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p pneumonia/trainer\n",
    "touch pneumonia/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2125feff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pneumonia/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pneumonia/trainer/task.py\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "from trainer import model\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--job-dir\",\n",
    "        help=\"this model ignores this field, but it is required by gcloud\",\n",
    "        default=\"junk\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_data_path\",\n",
    "        help=\"GCS location of training data\",\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--eval_data_path\",\n",
    "        help=\"GCS location of evaluation data\",\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        help=\"GCS location to write checkpoints and export models\",\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\",\n",
    "        help=\"Number of examples to compute gradient over.\",\n",
    "        type=int,\n",
    "        default=32\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--num_epochs\",\n",
    "        help=\"Number of epochs to train the model.\",\n",
    "        type=int,\n",
    "        default=10\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_examples\",\n",
    "        help=\"\"\"Number of examples (in thousands) to run the training job over.\n",
    "        If this is more than actual # of examples available, it cycles through\n",
    "        them. So specifying 1000 here when you have only 100k examples makes\n",
    "        this 10 epochs.\"\"\",\n",
    "        type=int,\n",
    "        default=5000\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--eval_steps\",\n",
    "        help=\"\"\"Positive number of steps for which to evaluate model. Default\n",
    "        to None, which means to evaluate until input_fn raises an end-of-input\n",
    "        exception\"\"\",\n",
    "        type=int,\n",
    "        default=None\n",
    "    )\n",
    "\n",
    "\n",
    "    # Parse all arguments\n",
    "    args = parser.parse_args()\n",
    "    arguments = args.__dict__\n",
    "\n",
    "    # Unused args provided by service\n",
    "    arguments.pop(\"job_dir\", None)\n",
    "    arguments.pop(\"job-dir\", None)\n",
    "\n",
    "    # Modify some arguments\n",
    "    arguments[\"train_examples\"] *= 1000\n",
    "\n",
    "    # Append trial_id to path if we are doing hptuning\n",
    "    # This code can be removed if you are not using hyperparameter tuning\n",
    "    arguments[\"output_dir\"] = os.path.join(\n",
    "        arguments[\"output_dir\"],\n",
    "        json.loads(\n",
    "            os.environ.get(\"TF_CONFIG\", \"{}\")\n",
    "        ).get(\"task\", {}).get(\"trial\", \"\")\n",
    "    )\n",
    "\n",
    "    # Run the training job\n",
    "    model.train_and_evaluate(arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57851d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pneumonia/trainer/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pneumonia/trainer/model.py\n",
    "import os\n",
    "import shutil\n",
    "import datetime\n",
    "\n",
    "import hypertune\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "\n",
    "#import cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.layers import Dense, Flatten, Softmax\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D,GlobalAveragePooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import logging\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "def load_data(train_path, val_path, batch_size):\n",
    "    \n",
    "    CLASS_LABELS = ['NORMAL', 'PNEUMONIA'] \n",
    "\n",
    "    def process_path(nb_class):\n",
    "    \n",
    "        def f(file_path):\n",
    "            \n",
    "            label = 0 if tf.strings.split(file_path, os.path.sep)[-2]=='NORMAL' else 1\n",
    "            \n",
    "            image = tf.io.read_file(file_path)    \n",
    "            image = tf.image.decode_jpeg(image, channels=3)\n",
    "            image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "         \n",
    "            image = tf.image.resize(image, [127, 127], method='area')\n",
    "            return image, label\n",
    "    \n",
    "        return f\n",
    "\n",
    "    def reader_image(path_file, batch_size, nb_class):\n",
    "\n",
    "        list_ds = tf.data.Dataset.list_files(path_file)\n",
    "        labeled_ds = list_ds.map(process_path(nb_class))\n",
    "    \n",
    "        return labeled_ds.shuffle(100).batch(batch_size).prefetch(1)\n",
    "    \n",
    "    train_ds = reader_image(train_path, batch_size, 2)\n",
    "    val_ds = reader_image(val_path, batch_size, 2)\n",
    "\n",
    "   # train_ds = reader_image('gs://chest-xray-us-central/chest_xray/train/*/*.jpeg', 16, 2)\n",
    "   # val_ds = reader_image('gs://chest-xray-us-central/chest_xray/test/*/*.jpeg', 16, 2)\n",
    "    print(type(train_ds))\n",
    "\n",
    "\n",
    "    for image, label in train_ds.take(1):\n",
    "        df = pd.DataFrame(image[0, :, :, 0].numpy())\n",
    "    \n",
    "    print(f'Outoupt : \\n image shape: {df.shape}')\n",
    "    \n",
    "    return train_ds, val_ds\n",
    "\n",
    "def train_and_evaluate(args):\n",
    "    from tensorflow.keras.applications.densenet import DenseNet169\n",
    "    from tensorflow.keras.applications.densenet import preprocess_input as densenet_preprocess\n",
    "\n",
    "    base = DenseNet169(weights = 'imagenet', include_top = False, input_shape = (127, 127, 3))\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    for layer in base.layers:\n",
    "        layer.trainable =  False \n",
    "\n",
    "    densenet_model = Sequential()\n",
    "    densenet_model.add(base)\n",
    "    densenet_model.add(GlobalAveragePooling2D())\n",
    "    densenet_model.add(BatchNormalization())\n",
    "    densenet_model.add(Dense(256, activation='relu'))\n",
    "    densenet_model.add(Dropout(0.5))\n",
    "    densenet_model.add(BatchNormalization())\n",
    "    densenet_model.add(Dense(128, activation='relu'))\n",
    "    densenet_model.add(Dropout(0.5))\n",
    "    densenet_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    densenet_model.summary()\n",
    "    \n",
    "    eval_steps = args[\"eval_steps\"]\n",
    "    \n",
    "    optm = Adam(lr=0.0001)\n",
    "    densenet_model.compile(loss='binary_crossentropy', optimizer=optm, \n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    checkpoint_path = os.path.join(args[\"output_dir\"], \"checkpoints/pneumonia\")\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_path, verbose=1, save_weights_only=True)\n",
    "    \n",
    "    train_ds, val_ds = load_data(args[\"train_data_path\"], args[\"eval_data_path\"], args[\"batch_size\"])\n",
    "  \n",
    "    dense_history = densenet_model.fit(\n",
    "              train_ds,\n",
    "              validation_data=val_ds,\n",
    "              epochs=args[\"num_epochs\"])\n",
    "    print(\"cheking the model run\")\n",
    "    \n",
    "    EXPORT_PATH = os.path.join(\n",
    "        args[\"output_dir\"], datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "    tf.saved_model.save(\n",
    "        obj=densenet_model, export_dir=EXPORT_PATH)\n",
    "    \n",
    "    print(\"Exported trained model to {}\".format(EXPORT_PATH))\n",
    "    \n",
    "\n",
    "    hp_metric = dense_history.history['val_accuracy'][eval_steps-1]\n",
    "    \n",
    "    hpt = hypertune.HyperTune()\n",
    "    hpt.report_hyperparameter_tuning_metric(\n",
    "        hyperparameter_metric_tag='accuracy',\n",
    "        metric_value=hp_metric,\n",
    "        global_step=eval_steps\n",
    "    )\n",
    "    return dense_history\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92e13459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process is terminated.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "OUTDIR=pneumonia_trained\n",
    "rm -rf ${OUTDIR}\n",
    "export PYTHONPATH=${PYTHONPATH}:${PWD}/pneumonia\n",
    "python3 -m trainer.task \\\n",
    "    --job-dir=./tmp \\\n",
    "    --train_data_path=gs://${BUCKET}/chest_xray/train/*/*.jpeg \\\n",
    "    --eval_data_path=gs://${BUCKET}/chest_xray/test/*/*.jpeg \\\n",
    "    --output_dir=${OUTDIR} \\\n",
    "    --batch_size=4 \\\n",
    "    --num_epochs=1 \\\n",
    "    --train_examples=1 \\\n",
    "    --eval_steps=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "54a79f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobId: pneumonia_210624_041649\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Job [pneumonia_210624_041649] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe pneumonia_210624_041649\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs pneumonia_210624_041649\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "OUTDIR=gs://${BUCKET}/pneumonia/trained_model\n",
    "JOBID=pneumonia_$(date -u +%y%m%d_%H%M%S)\n",
    "\n",
    "gcloud ai-platform jobs submit training ${JOBID} \\\n",
    "    --region=${REGION} \\\n",
    "    --module-name=trainer.task \\\n",
    "    --package-path=$(pwd)/pneumonia/trainer \\\n",
    "    --job-dir=${OUTDIR} \\\n",
    "    --staging-bucket=gs://${BUCKET} \\\n",
    "    --master-machine-type=n1-standard-8 \\\n",
    "    --scale-tier=CUSTOM \\\n",
    "    --runtime-version=${TFVERSION} \\\n",
    "    --python-version=${PYTHONVERSION} \\\n",
    "    -- \\\n",
    "    --train_data_path=gs://${BUCKET}/chest_xray/train/*/*.jpeg \\\n",
    "    --eval_data_path=gs://${BUCKET}/chest_xray/test/*/*.jpeg \\\n",
    "    --output_dir=${OUTDIR} \\\n",
    "    --num_epochs=1 \\\n",
    "    --batch_size=32 \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "495bd02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hyperparam.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile hyperparam.yaml\n",
    "trainingInput:\n",
    "    scaleTier: STANDARD_1\n",
    "    hyperparameters:\n",
    "        hyperparameterMetricTag: accuracy\n",
    "        goal: MAXIMIZE\n",
    "        maxTrials: 20\n",
    "        maxParallelTrials: 5\n",
    "        enableTrialEarlyStopping: True\n",
    "        params:\n",
    "        - parameterName: batch_size\n",
    "          type: INTEGER\n",
    "          minValue: 8\n",
    "          maxValue: 512\n",
    "          scaleType: UNIT_LOG_SCALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2726afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://chest-xray-us-central/pneumonia/hyperparam us-central1 pneumonia_210624_143333\n",
      "jobId: pneumonia_210624_143333\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removing gs://chest-xray-us-central/pneumonia/hyperparam/#1624544624546399...\n",
      "Removing gs://chest-xray-us-central/pneumonia/hyperparam/1/#1624544624738746...\n",
      "Removing gs://chest-xray-us-central/pneumonia/hyperparam/1/20210624142241/#1624544624958532...\n",
      "Removing gs://chest-xray-us-central/pneumonia/hyperparam/1/20210624142241/assets/#1624544627952308...\n",
      "Removing gs://chest-xray-us-central/pneumonia/hyperparam/1/20210624142241/saved_model.pb#1624544632145147...\n",
      "Removing gs://chest-xray-us-central/pneumonia/hyperparam/1/20210624142241/variables/#1624544625148010...\n",
      "Removing gs://chest-xray-us-central/pneumonia/hyperparam/1/20210624142241/variables/variables.data-00000-of-00001#1624544627251267...\n",
      "Removing gs://chest-xray-us-central/pneumonia/hyperparam/1/20210624142241/variables/variables.index#1624544627435226...\n",
      "/ [8/8 objects] 100% Done                                                       \n",
      "Operation completed over 8 objects.                                              \n",
      "Job [pneumonia_210624_143333] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe pneumonia_210624_143333\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs pneumonia_210624_143333\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "OUTDIR=gs://${BUCKET}/pneumonia/hyperparam\n",
    "JOBNAME=pneumonia_$(date -u +%y%m%d_%H%M%S)\n",
    "echo ${OUTDIR} ${REGION} ${JOBNAME}\n",
    "gsutil -m rm -rf ${OUTDIR}\n",
    "\n",
    "gcloud ai-platform jobs submit training ${JOBNAME} \\\n",
    "    --staging-bucket=gs://${BUCKET} \\\n",
    "    --region=${REGION} \\\n",
    "    --module-name=trainer.task \\\n",
    "    --package-path=$(pwd)/pneumonia/trainer \\\n",
    "    --master-machine-type=n1-standard-8 \\\n",
    "    --runtime-version=${TFVERSION} \\\n",
    "    --python-version=${PYTHONVERSION} \\\n",
    "    --scale-tier=CUSTOM \\\n",
    "    --config=hyperparam.yaml \\\n",
    "    -- \\\n",
    "    --train_data_path=gs://${BUCKET}/chest_xray/train/*/*.jpeg \\\n",
    "    --eval_data_path=gs://${BUCKET}/chest_xray/test/*/*.jpeg \\\n",
    "    --output_dir=${OUTDIR} \\\n",
    "    --num_epochs=20 \\\n",
    "    --eval_steps=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8dd6dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c269ad3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f756b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416a2a38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8440834a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m71",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m71"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
