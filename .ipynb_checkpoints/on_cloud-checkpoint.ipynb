{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22c49fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cloudml-hypertune in /opt/conda/lib/python3.7/site-packages (0.1.0.dev6)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install cloudml-hypertune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd50a1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7805bf50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your current GCP Project Name is: qwiklabs-gcp-04-014eea1283f0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export PROJECT=$(gcloud config list project --format \"value(core.project)\")\n",
    "echo \"Your current GCP Project Name is: \"${PROJECT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ba976b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Change these to try this notebook out\n",
    "PROJECT = \"qwiklabs-gcp-04-014eea1283f0\"  # Replace with your PROJECT\n",
    "BUCKET = \"chest-xray-us-central\"   # defaults to PROJECT\n",
    "REGION = \"us-central1\"  # Replace with your REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5035ee20",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"BUCKET\"] = BUCKET\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"TFVERSION\"] = \"2.1\"\n",
    "os.environ[\"PYTHONVERSION\"] = \"3.7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "839d2a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [compute/region].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud config set project ${PROJECT}\n",
    "gcloud config set compute/region ${REGION}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c6a2b9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if ! gsutil ls | grep -q gs://${BUCKET}; then\n",
    "    gsutil mb -l ${REGION} gs://${BUCKET}\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "66b81c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://chest-xray-us-central/chest_xray/chest_xray_labels.csv\n",
      "gs://chest-xray-us-central/chest_xray/test/\n",
      "gs://chest-xray-us-central/chest_xray/train/\n",
      "gs://chest-xray-us-central/chest_xray/val/\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gsutil ls gs://${BUCKET}/chest_xray/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9dd6ba2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p pneumonia/trainer\n",
    "touch pneumonia/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3bfc6ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pneumonia/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pneumonia/trainer/task.py\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "from trainer import model\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--job-dir\",\n",
    "        help=\"this model ignores this field, but it is required by gcloud\",\n",
    "        default=\"junk\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_data_path\",\n",
    "        help=\"GCS location of training data\",\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--eval_data_path\",\n",
    "        help=\"GCS location of evaluation data\",\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        help=\"GCS location to write checkpoints and export models\",\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\",\n",
    "        help=\"Number of examples to compute gradient over.\",\n",
    "        type=int,\n",
    "        default=32\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--num_epochs\",\n",
    "        help=\"Number of epochs to train the model.\",\n",
    "        type=int,\n",
    "        default=10\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_examples\",\n",
    "        help=\"\"\"Number of examples (in thousands) to run the training job over.\n",
    "        If this is more than actual # of examples available, it cycles through\n",
    "        them. So specifying 1000 here when you have only 100k examples makes\n",
    "        this 10 epochs.\"\"\",\n",
    "        type=int,\n",
    "        default=5000\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--eval_steps\",\n",
    "        help=\"\"\"Positive number of steps for which to evaluate model. Default\n",
    "        to None, which means to evaluate until input_fn raises an end-of-input\n",
    "        exception\"\"\",\n",
    "        type=int,\n",
    "        default=None\n",
    "    )\n",
    "\n",
    "\n",
    "    # Parse all arguments\n",
    "    args = parser.parse_args()\n",
    "    arguments = args.__dict__\n",
    "\n",
    "    # Unused args provided by service\n",
    "    arguments.pop(\"job_dir\", None)\n",
    "    arguments.pop(\"job-dir\", None)\n",
    "\n",
    "    # Modify some arguments\n",
    "    arguments[\"train_examples\"] *= 1000\n",
    "\n",
    "    # Append trial_id to path if we are doing hptuning\n",
    "    # This code can be removed if you are not using hyperparameter tuning\n",
    "    arguments[\"output_dir\"] = os.path.join(\n",
    "        arguments[\"output_dir\"],\n",
    "        json.loads(\n",
    "            os.environ.get(\"TF_CONFIG\", \"{}\")\n",
    "        ).get(\"task\", {}).get(\"trial\", \"\")\n",
    "    )\n",
    "\n",
    "    # Run the training job\n",
    "    model.train_and_evaluate(arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e8e6cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pneumonia/trainer/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pneumonia/trainer/model.py\n",
    "import os\n",
    "import shutil\n",
    "import datetime\n",
    "\n",
    "import hypertune\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "\n",
    "#import cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.layers import Dense, Flatten, Softmax\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D,GlobalAveragePooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import logging\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "def load_data(train_path, val_path, batch_size):\n",
    "    \n",
    "    CLASS_LABELS = ['NORMAL', 'PNEUMONIA'] \n",
    "\n",
    "    def process_path(nb_class):\n",
    "    \n",
    "        def f(file_path):\n",
    "            \n",
    "            label = 0 if tf.strings.split(file_path, os.path.sep)[-2]=='NORMAL' else 1\n",
    "            \n",
    "            image = tf.io.read_file(file_path)    \n",
    "            image = tf.image.decode_jpeg(image, channels=3)\n",
    "            image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "         \n",
    "            image = tf.image.resize(image, [127, 127], method='area')\n",
    "            return image, label\n",
    "    \n",
    "        return f\n",
    "\n",
    "    def reader_image(path_file, batch_size, nb_class):\n",
    "\n",
    "        list_ds = tf.data.Dataset.list_files(path_file)\n",
    "        labeled_ds = list_ds.map(process_path(nb_class))\n",
    "    \n",
    "        return labeled_ds.shuffle(100).batch(batch_size).prefetch(1)\n",
    "    \n",
    "    train_ds = reader_image(train_path, batch_size, 2)\n",
    "    val_ds = reader_image(val_path, batch_size, 2)\n",
    "\n",
    "   # train_ds = reader_image('gs://chest-xray-us-central/chest_xray/train/*/*.jpeg', 16, 2)\n",
    "   # val_ds = reader_image('gs://chest-xray-us-central/chest_xray/test/*/*.jpeg', 16, 2)\n",
    "    print(type(train_ds))\n",
    "\n",
    "\n",
    "    for image, label in train_ds.take(1):\n",
    "        df = pd.DataFrame(image[0, :, :, 0].numpy())\n",
    "    \n",
    "    print(f'Outoupt : \\n image shape: {df.shape}')\n",
    "    \n",
    "    return train_ds, val_ds\n",
    "\n",
    "def train_and_evaluate(args):\n",
    "    from tensorflow.keras.applications.densenet import DenseNet169\n",
    "    from tensorflow.keras.applications.densenet import preprocess_input as densenet_preprocess\n",
    "\n",
    "    base = DenseNet169(weights = 'imagenet', include_top = False, input_shape = (127, 127, 3))\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    for layer in base.layers:\n",
    "        layer.trainable =  False \n",
    "\n",
    "    densenet_model = Sequential()\n",
    "    densenet_model.add(base)\n",
    "    densenet_model.add(GlobalAveragePooling2D())\n",
    "    densenet_model.add(BatchNormalization())\n",
    "    densenet_model.add(Dense(256, activation='relu'))\n",
    "    densenet_model.add(Dropout(0.5))\n",
    "    densenet_model.add(BatchNormalization())\n",
    "    densenet_model.add(Dense(128, activation='relu'))\n",
    "    densenet_model.add(Dropout(0.5))\n",
    "    densenet_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    densenet_model.summary()\n",
    "    \n",
    "    eval_steps = args[\"eval_steps\"]\n",
    "    \n",
    "    optm = Adam(lr=0.0001)\n",
    "    densenet_model.compile(loss='binary_crossentropy', optimizer=optm, \n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    checkpoint_path = os.path.join(args[\"output_dir\"], \"checkpoints/pneumonia\")\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_path, verbose=1, save_weights_only=True)\n",
    "    \n",
    "    train_ds, val_ds = load_data(args[\"train_data_path\"], args[\"eval_data_path\"], args[\"batch_size\"])\n",
    "  \n",
    "    dense_history = densenet_model.fit(\n",
    "              train_ds,\n",
    "              validation_data=val_ds,\n",
    "              epochs=args[\"num_epochs\"])\n",
    "    print(\"cheking the model run\")\n",
    "    \n",
    "    EXPORT_PATH = os.path.join(\n",
    "        args[\"output_dir\"], datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "    tf.saved_model.save(\n",
    "        obj=densenet_model, export_dir=EXPORT_PATH)\n",
    "    \n",
    "    print(\"Exported trained model to {}\".format(EXPORT_PATH))\n",
    "    \n",
    "\n",
    "    hp_metric = dense_history.history['val_accuracy'][eval_steps-1]\n",
    "    \n",
    "    hpt = hypertune.HyperTune()\n",
    "    hpt.report_hyperparameter_tuning_metric(\n",
    "        hyperparameter_metric_tag='accuracy',\n",
    "        metric_value=hp_metric,\n",
    "        global_step=eval_steps\n",
    "    )\n",
    "    return dense_history\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6abccc51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "densenet169 (Functional)     (None, 4, 4, 1664)        12642880  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 1664)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 1664)              6656      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               426240    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 13,109,825\n",
      "Trainable params: 463,105\n",
      "Non-trainable params: 12,646,720\n",
      "_________________________________________________________________\n",
      "<class 'tensorflow.python.data.ops.dataset_ops.PrefetchDataset'>\n",
      "Outoupt : \n",
      " image shape: (127, 127)\n",
      "1304/1304 [==============================] - 270s 198ms/step - loss: 0.4318 - accuracy: 0.8012 - val_loss: 0.4554 - val_accuracy: 0.8125\n",
      "cheking the model run\n",
      "Exported trained model to pneumonia_trained/20210624025539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-24 02:50:54.169497: E tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/core/saved_model/write/count\n",
      "2021-06-24 02:50:54.169546: E tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/core/saved_model/read/count\n",
      "2021-06-24 02:50:54.169554: E tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/core/saved_model/write/api\n",
      "2021-06-24 02:50:54.169560: E tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/core/saved_model/read/api\n",
      "2021-06-24 02:50:55.555049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-24 02:50:55.555568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-24 02:50:55.561947: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2021-06-24 02:50:55.562891: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1728] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-06-24 02:50:55.563145: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-06-24 02:51:04.339924: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:164] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-06-24 02:56:07.135718: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "OUTDIR=pneumonia_trained\n",
    "rm -rf ${OUTDIR}\n",
    "export PYTHONPATH=${PYTHONPATH}:${PWD}/pneumonia\n",
    "python3 -m trainer.task \\\n",
    "    --job-dir=./tmp \\\n",
    "    --train_data_path=gs://${BUCKET}/chest_xray/train/*/*.jpeg \\\n",
    "    --eval_data_path=gs://${BUCKET}/chest_xray/test/*/*.jpeg \\\n",
    "    --output_dir=${OUTDIR} \\\n",
    "    --batch_size=4 \\\n",
    "    --num_epochs=1 \\\n",
    "    --train_examples=1 \\\n",
    "    --eval_steps=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4b694ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobId: pneumonia_210624_041649\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Job [pneumonia_210624_041649] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe pneumonia_210624_041649\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs pneumonia_210624_041649\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "OUTDIR=gs://${BUCKET}/pneumonia/trained_model\n",
    "JOBID=pneumonia_$(date -u +%y%m%d_%H%M%S)\n",
    "\n",
    "gcloud ai-platform jobs submit training ${JOBID} \\\n",
    "    --region=${REGION} \\\n",
    "    --module-name=trainer.task \\\n",
    "    --package-path=$(pwd)/pneumonia/trainer \\\n",
    "    --job-dir=${OUTDIR} \\\n",
    "    --staging-bucket=gs://${BUCKET} \\\n",
    "    --master-machine-type=n1-standard-8 \\\n",
    "    --scale-tier=CUSTOM \\\n",
    "    --runtime-version=${TFVERSION} \\\n",
    "    --python-version=${PYTHONVERSION} \\\n",
    "    -- \\\n",
    "    --train_data_path=gs://${BUCKET}/chest_xray/train/*/*.jpeg \\\n",
    "    --eval_data_path=gs://${BUCKET}/chest_xray/test/*/*.jpeg \\\n",
    "    --output_dir=${OUTDIR} \\\n",
    "    --num_epochs=1 \\\n",
    "    --batch_size=32 \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad40efd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hyperparam.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile hyperparam.yaml\n",
    "trainingInput:\n",
    "    scaleTier: STANDARD_1\n",
    "    hyperparameters:\n",
    "        hyperparameterMetricTag: accuracy\n",
    "        goal: MAXIMIZE\n",
    "        maxTrials: 20\n",
    "        maxParallelTrials: 5\n",
    "        enableTrialEarlyStopping: True\n",
    "        params:\n",
    "        - parameterName: batch_size\n",
    "          type: INTEGER\n",
    "          minValue: 8\n",
    "          maxValue: 512\n",
    "          scaleType: UNIT_LOG_SCALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6abedf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://chest-xray-us-central/pneumonia/hyperparam us-central1 pneumonia_210624_075318\n",
      "jobId: pneumonia_210624_075318\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removing gs://chest-xray-us-central/pneumonia/hyperparam/#1624519814453494...\n",
      "Removing gs://chest-xray-us-central/pneumonia/hyperparam/1/#1624519844853774...\n",
      "Removing gs://chest-xray-us-central/pneumonia/hyperparam/1/20210624072938/#1624519845048769...\n",
      "Removing gs://chest-xray-us-central/pneumonia/hyperparam/1/20210624072938/assets/#1624519848324715...\n",
      "Removing gs://chest-xray-us-central/pneumonia/hyperparam/1/20210624072938/saved_model.pb#1624519853808637...\n",
      "Removing gs://chest-xray-us-central/pneumonia/hyperparam/1/20210624072938/variables/#1624519845299721...\n",
      "Removing gs://chest-xray-us-central/pneumonia/hyperparam/1/20210624072938/variables/variables.data-00000-of-00001#1624519847565767...\n",
      "Removing gs://chest-xray-us-central/pneumonia/hyperparam/1/20210624072938/variables/variables.index#1624519847781848...\n",
      "Removing gs://chest-xray-us-central/pneumonia/hyperparam/2/#1624519814649521...\n",
      "Removing gs://chest-xray-us-central/pneumonia/hyperparam/2/20210624072907/#1624519814828296...\n",
      "Removing gs://chest-xray-us-central/pneumonia/hyperparam/2/20210624072907/assets/#1624519817962447...\n",
      "Removing gs://chest-xray-us-central/pneumonia/hyperparam/2/20210624072907/saved_model.pb#1624519822320605...\n",
      "Removing gs://chest-xray-us-central/pneumonia/hyperparam/2/20210624072907/variables/#1624519815042160...\n",
      "Removing gs://chest-xray-us-central/pneumonia/hyperparam/2/20210624072907/variables/variables.data-00000-of-00001#1624519817234400...\n",
      "Removing gs://chest-xray-us-central/pneumonia/hyperparam/2/20210624072907/variables/variables.index#1624519817420404...\n",
      "Removing gs://chest-xray-us-central/pneumonia/hyperparam/3/#1624520047750405...\n",
      "Removing gs://chest-xray-us-central/pneumonia/hyperparam/3/20210624073252/#1624520048024516...\n",
      "Removing gs://chest-xray-us-central/pneumonia/hyperparam/3/20210624073252/assets/#1624520051627509...\n",
      "Removing gs://chest-xray-us-central/pneumonia/hyperparam/3/20210624073252/saved_model.pb#1624520056312673...\n",
      "Removing gs://chest-xray-us-central/pneumonia/hyperparam/3/20210624073252/variables/variables.data-00000-of-00001#1624520050754973...\n",
      "Removing gs://chest-xray-us-central/pneumonia/hyperparam/3/20210624073252/variables/variables.index#1624520050946664...\n",
      "Removing gs://chest-xray-us-central/pneumonia/hyperparam/3/20210624073252/variables/#1624520048279920...\n",
      "Removing gs://chest-xray-us-central/pneumonia/hyperparam/4/#1624519883748418...\n",
      "Removing gs://chest-xray-us-central/pneumonia/hyperparam/4/20210624073018/#1624519883964570...\n",
      "Removing gs://chest-xray-us-central/pneumonia/hyperparam/4/20210624073018/assets/#1624519887643604...\n",
      "Removing gs://chest-xray-us-central/pneumonia/hyperparam/4/20210624073018/saved_model.pb#1624519892057794...\n",
      "Removing gs://chest-xray-us-central/pneumonia/hyperparam/5/20210624073225/#1624520013505168...\n",
      "Removing gs://chest-xray-us-central/pneumonia/hyperparam/4/20210624073018/variables/#1624519884206912...\n",
      "Removing gs://chest-xray-us-central/pneumonia/hyperparam/5/20210624073225/assets/#1624520016811490...\n",
      "Removing gs://chest-xray-us-central/pneumonia/hyperparam/4/20210624073018/variables/variables.data-00000-of-00001#1624519886860448...\n",
      "Removing gs://chest-xray-us-central/pneumonia/hyperparam/5/20210624073225/variables/#1624520013727967...\n",
      "Removing gs://chest-xray-us-central/pneumonia/hyperparam/4/20210624073018/variables/variables.index#1624519887041913...\n",
      "Removing gs://chest-xray-us-central/pneumonia/hyperparam/5/20210624073225/saved_model.pb#1624520021389483...\n",
      "Removing gs://chest-xray-us-central/pneumonia/hyperparam/5/#1624520013316957...\n",
      "Removing gs://chest-xray-us-central/pneumonia/hyperparam/5/20210624073225/variables/variables.data-00000-of-00001#1624520016053081...\n",
      "Removing gs://chest-xray-us-central/pneumonia/hyperparam/5/20210624073225/variables/variables.index#1624520016216320...\n",
      "/ [36/36 objects] 100% Done                                                     \n",
      "Operation completed over 36 objects.                                             \n",
      "Job [pneumonia_210624_075318] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe pneumonia_210624_075318\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs pneumonia_210624_075318\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "OUTDIR=gs://${BUCKET}/pneumonia/hyperparam\n",
    "JOBNAME=pneumonia_$(date -u +%y%m%d_%H%M%S)\n",
    "echo ${OUTDIR} ${REGION} ${JOBNAME}\n",
    "gsutil -m rm -rf ${OUTDIR}\n",
    "\n",
    "gcloud ai-platform jobs submit training ${JOBNAME} \\\n",
    "    --staging-bucket=gs://${BUCKET} \\\n",
    "    --region=${REGION} \\\n",
    "    --module-name=trainer.task \\\n",
    "    --package-path=$(pwd)/pneumonia/trainer \\\n",
    "    --master-machine-type=n1-standard-8 \\\n",
    "    --runtime-version=${TFVERSION} \\\n",
    "    --python-version=${PYTHONVERSION} \\\n",
    "    --scale-tier=CUSTOM \\\n",
    "    --config=hyperparam.yaml \\\n",
    "    -- \\\n",
    "    --train_data_path=gs://${BUCKET}/chest_xray/train/*/*.jpeg \\\n",
    "    --eval_data_path=gs://${BUCKET}/chest_xray/test/*/*.jpeg \\\n",
    "    --output_dir=${OUTDIR} \\\n",
    "    --num_epochs=1 \\\n",
    "    --eval_steps=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a5b097",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ce8b7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c8aef4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e32cb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33de55f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m71",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m71"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
